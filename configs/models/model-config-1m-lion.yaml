name: "Micro-1M-Lion"
data:
  input_file: "datasets/clean_training_data/training_data.txt"
  tokenizer_path: "tokenizer"
  preprocessing:
    max_context_size: 1024
    chunk_overlap: 128
  tokenizer:
    normal_vocab_size: 256  # Default to byte-level
    special_tokens:
      pad: "<pad>"
      bos: "<s>"
      eos: "</s>"
      unk: "<unk>"

model:
  architecture: "llama"
  dimensions:
    hidden_size: 256
    num_layers: 8
    intermediate_size: 768
  attention:
    num_heads: 4
    num_kv_heads: 4
    head_dim: 64
    max_position_embeddings: 2048
    use_flash_attention: true
    use_flex_attention: false
    flash_block_size: 128
  normalization:
    rms_norm_eps: 1.0e-5
  rope:
    theta: 10000.0
    traditional: false
    scaling: null
  misc:
    attention_bias: false
    mlp_bias: false
    tie_word_embeddings: false

training:
  hyperparameters:
    learning_rate: 5.0e-4
    weight_decay: 0.01
    batch_size: 32
    gradient_clip: 1.0
    gradient_accumulation_steps: 1
  scheduler:
    type: "cosine_with_warmup"
    warmup_steps: 100
    min_lr_ratio: 0.1
  optimization:
    optimizer: "lion"
    betas: [0.9, 0.95]
    eps: 1.0e-8
    grad_clip_norm: 1.0
    ema_momentum: 0.9999
  epochs: 30

logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  steps:
    logging_interval: 10
    checkpoint_interval: 500
    validation_interval: 200
  metrics:
    log_loss: true
    log_perplexity: true
    log_tokens_per_second: true
    log_tokens_processed: true
    log_learning_rate: true

system:
  seed: 42
  device: "gpu"
  distributed: false
  memory_limit: null