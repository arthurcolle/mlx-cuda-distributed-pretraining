name: "BPE Tokenizer Training"
data:
  input_file: "processed_dataset/train.jsonl"
  max_texts_to_train_on: 32768
  tokenizer:
    special_tokens:
      pad: "<pad>"
      bos: "<bos>"
      eos: "<eos>"

tokenizer:
  vocab_size: 200000
  output_dir: "tokenizer"
  type: "modern_bpe"
