name: "Micro-1M-adamw-20250426_013635"
overwrite: true
data:
  input_file: "train.jsonl"
  validation_file: "val.jsonl"
  tokenizer_path: "tokenizer"
  preprocessing:
    max_context_size: 512  # Reduced context size for faster training
    chunk_overlap: 64
    
  tokenizer:
    normal_vocab_size: 32000
    special_tokens:
      pad: "<pad>"
      bos: "<bos>"
      eos: "<eos>"

model:
  architecture: "llama"
  dimensions:
    hidden_size: 128  # Very small hidden size for 1M model
    intermediate_size: 512  # Reduced intermediate size
    num_layers: 6  # Fewer layers
  attention:
    num_heads: 4  # Reduced number of heads
    num_kv_heads: 2  # GQA for efficiency
    head_dim: 32  # Smaller head dimension
    max_position_embeddings: 512  # Match context size
    use_flash_attention: false
    flash_block_size: 64
  normalization:
    rms_norm_eps: 1.0e-5
  rope:
    theta: 10000
    traditional: false
    scaling: null
  misc:
    attention_bias: false
    mlp_bias: false
    tie_word_embeddings: true

training:
  epochs: null
  hyperparameters:
    batch_size: 32  # Higher base batch size for small model
    gradient_accumulation_steps: 2  # Lower accumulation for faster iterations
    learning_rate: 5.0e-3  # Higher learning rate for faster convergence
    weight_decay: 0.01
    gradient_clip: 1.0
    iters: 2000  # Fewer iterations for quick experiments
    
  scheduler:
    type: "cosine_with_warmup"
    min_lr_ratio: 0.1
    warmup_steps: 200
    
  optimization:
    # Change between "muon", "shampoo", and "adamw" to compare
    optimizer: "adamw"
    # Muon parameters
    momentum: 0.95
    nesterov: true
    ns_steps: 5
    # Shampoo parameters (used when optimizer is "shampoo")
    update_period: 50  # Update preconditioners more frequently for small model
    start_preconditioning_step: 100
    preconditioner_epsilon: 1.0e-6
    exponent_override: 0.75
    beta1: 0.9
    beta2: 0.95
    epsilon: 1.0e-8
    grafting_optimizer: "adam"

logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints/micro-1m"
  steps:
    logging_interval: 1  # Log very frequently for clearer plots
    checkpoint_interval: 200
    validation_interval: 100
  metrics:
    log_loss: true
    log_perplexity: true
    log_tokens_per_second: true
    log_learning_rate: true
    log_tokens_processed: true

system:
  seed: 42
  device: "gpu"
  distributed: false  # Simpler single-device training for fast iteration
  devices: ["mlx"]  # Use MLX locally
  cuda_devices: []  # No CUDA for fast experimentation